{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Рубежный контроль №2\n",
    "## <center> \"Технологии использования и оценки моделей машинного обучения\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Вариант 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### Задание:\n",
    "   \n",
    "Задача 1. Классификация текстов на основе методов наивного Байеса.\n",
    "\n",
    "    Необходимо решить задачу классификации текстов на основе любого выбранного Вами датасета (кроме примера, который рассматривался в лекции). Классификация может быть бинарной или многоклассовой. Целевой признак из выбранного Вами датасета может иметь любой физический смысл, примером является задача анализа тональности текста.\n",
    "\n",
    "    Необходимо сформировать признаки на основе CountVectorizer или TfidfVectorizer.\n",
    "\n",
    "    В качестве классификаторов необходимо использовать два классификатора, не относящихся к наивным Байесовским методам (например, LogisticRegression, LinearSVC), а также Multinomial Naive Bayes (MNB), Complement Naive Bayes (CNB), Bernoulli Naive Bayes.\n",
    "\n",
    "    Для каждого метода необходимо оценить качество классификации с помощью хотя бы одной метрики качества классификации например, Accuracy).\n",
    "\n",
    "    Сделате выводы о том, какой классификатор осуществляет более качественную классификацию на Вашем наборе данных.\n",
    "\n",
    "Задача 2. Кластеризация данных\n",
    "\n",
    "    Кластеризуйте данные с помощью двух алгоритмов кластеризации: K-Means (k-means++) \tBirch\n",
    "\n",
    "Сравните качество кластеризации с помощью следующих метрик качества кластеризации (если это возможно для Вашего набора данных):\n",
    "\n",
    "    Adjusted Rand index\n",
    "    Adjusted Mutual Information\n",
    "    Homogeneity, completeness, V-measure\n",
    "    Коэффициент силуэта\n",
    "\n",
    "Сделате выводы о том, какой алгоритм осуществляет более качественную кластеризацию на Вашем наборе данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Tuple\n",
    "from scipy import stats\n",
    "from IPython.display import Image\n",
    "from sklearn.datasets import load_iris, load_boston\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error, median_absolute_error, r2_score \n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.svm import SVC, NuSVC, LinearSVC, OneClassSVM, SVR, NuSVR, LinearSVR\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "sns.set(style=\"ticks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score_for_classes(\n",
    "    y_true: np.ndarray, \n",
    "    y_pred: np.ndarray) -> Dict[int, float]:\n",
    "    \"\"\"\n",
    "    Вычисление метрики accuracy для каждого класса\n",
    "    y_true - истинные значения классов\n",
    "    y_pred - предсказанные значения классов\n",
    "    Возвращает словарь: ключ - метка класса, \n",
    "    значение - Accuracy для данного класса\n",
    "    \"\"\"\n",
    "    # Для удобства фильтрации сформируем Pandas DataFrame \n",
    "    d = {'t': y_true, 'p': y_pred}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    # Метки классов\n",
    "    classes = np.unique(y_true)\n",
    "    # Результирующий словарь\n",
    "    res = dict()\n",
    "    # Перебор меток классов\n",
    "    for c in classes:\n",
    "        # отфильтруем данные, которые соответствуют \n",
    "        # текущей метке класса в истинных значениях\n",
    "        temp_data_flt = df[df['t']==c]\n",
    "        # расчет accuracy для заданной метки класса\n",
    "        temp_acc = accuracy_score(\n",
    "            temp_data_flt['t'].values, \n",
    "            temp_data_flt['p'].values)\n",
    "        # сохранение результата в словарь\n",
    "        res[c] = temp_acc\n",
    "    return res\n",
    "\n",
    "def print_accuracy_score_for_classes(\n",
    "    y_true: np.ndarray, \n",
    "    y_pred: np.ndarray):\n",
    "    \"\"\"\n",
    "    Вывод метрики accuracy для каждого класса\n",
    "    \"\"\"\n",
    "    accs = accuracy_score_for_classes(y_true, y_pred)\n",
    "    if len(accs)>0:\n",
    "        print('Метка \\t Accuracy')\n",
    "    for i in accs:\n",
    "        print('{} \\t {}'.format(i, accs[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача 1. Классификация текстов на основе методов наивного Байеса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Использованный набор данных\n",
    "\n",
    "Для лабораторной работы будем использовать набор данных об [отзывах на Амазон](https://www.kaggle.com/marklvl/sentiment-labelled-sentences-data-set)\n",
    "\n",
    "Колонки:\n",
    "\n",
    " - `text` - текст отзыва\n",
    " - `value` - значение отзыва\n",
    " - `Unnamed: 2` - неизвестный параметр 1\n",
    " - `Unnamed: 3` - неизвестный параметр 2\n",
    " - `Unnamed: 4` - неизвестный параметр 3\n",
    " - `Unnamed: 5` - неизвестный параметр 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  value\n",
       "0  So there is no way for me to plug it in here i...      0\n",
       "1                        Good case, Excellent value.      1\n",
       "2                             Great for the jawbone.      1\n",
       "3  Tied to charger for conversations lasting more...      0\n",
       "4                                  The mic is great.      1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Таблица данных\n",
    "data = pd.read_csv(\"../Dataset/amazon_cells_labelled.txt\",  delimiter= '\\t',  header= None,  names= ['text',  'value']) \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Размер набора данных\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     object\n",
       "value     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Типы данных в колонках\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Проверка на наличие пропущенных значений**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "value    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Векторизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сформируем общий словарь для обучения моделей из обучающей и тестовой выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good case, Excellent value.',\n",
       " 'Great for the jawbone.',\n",
       " 'Tied to charger for conversations lasting more than 45 minutes.MAJOR PROBLEMS!!',\n",
       " 'The mic is great.',\n",
       " 'I have to jiggle the plug to get it to line up right to get decent volume.',\n",
       " 'If you have several dozen or several hundred contacts, then imagine the fun of sending each of them one by one.',\n",
       " 'If you are Razr owner...you must have this!',\n",
       " 'Needless to say, I wasted my money.',\n",
       " 'What a waste of money and time!.']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list = data['text'].tolist()\n",
    "vocab_list[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Создание векторизатора:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество сформированных признаков - 1847\n"
     ]
    }
   ],
   "source": [
    "vocabVect = CountVectorizer()\n",
    "vocabVect.fit(vocab_list)\n",
    "corpusVocab = vocabVect.vocabulary_\n",
    "print('Количество сформированных признаков - {}'.format(len(corpusVocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there=1609\n",
      "is=854\n",
      "no=1074\n",
      "way=1766\n",
      "for=653\n",
      "me=993\n",
      "to=1640\n",
      "plug=1212\n",
      "it=857\n"
     ]
    }
   ],
   "source": [
    "for i in list(corpusVocab)[1:10]:\n",
    "    print('{}={}'.format(i, corpusVocab[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала в словаре идут все номера, которые попадались в спам-сообщениях:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2005', '2160', '24', '2mp', '325', '350', '375', '3o', '42', '44']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabVect.get_feature_names()[10:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После них уже идут все обнаруженные слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['memory',\n",
       " 'mention',\n",
       " 'mentioned',\n",
       " 'menus',\n",
       " 'mere',\n",
       " 'mess',\n",
       " 'message',\n",
       " 'messages',\n",
       " 'messaging',\n",
       " 'messes']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabVect.get_feature_names()[1000:1010]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Использование N-грам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим словарь, содержащий словосочетания, состоящие из 1, 2, 3 слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обнаружено словосочетаний: 15088\n"
     ]
    }
   ],
   "source": [
    "ncv = CountVectorizer(ngram_range=(1,3))\n",
    "ngram_features = ncv.fit_transform(vocab_list)\n",
    "print('Обнаружено словосочетаний:',len(ncv.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Примеры словосочетаний:\n",
      "['any damage', 'any from', 'any from that', 'any helpful', 'any helpful support', 'any kind', 'any kind of', 'any large', 'any large problems', 'any longer'] \n",
      "\n",
      "['to do anything', 'to do everything', 'to do hard', 'to do voice', 'to download', 'to download the', 'to everyone', 'to exchange', 'to exchange bad', 'to find']\n"
     ]
    }
   ],
   "source": [
    "print (\"Примеры словосочетаний:\")\n",
    "print(ncv.get_feature_names()[1000:1010],'\\n')\n",
    "print(ncv.get_feature_names()[13000:13010])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Решение задачи классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VectorizeAndClassify(vectorizers_list, classifiers_list):\n",
    "    for v in vectorizers_list:\n",
    "        for c in classifiers_list:\n",
    "            pipeline1 = Pipeline([(\"vectorizer\", v), (\"classifier\", c)])\n",
    "            score = cross_val_score(pipeline1, data['text'], data['value'], scoring='accuracy', cv=3).mean()\n",
    "            print('Векторизация - {}'.format(v))\n",
    "            print('Модель для классификации - {}'.format(c))\n",
    "            print('Accuracy = {}'.format(score))\n",
    "            print('===========================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Векторизация - CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None,\n",
      "                vocabulary={'10': 0, '100': 1, '11': 2, '12': 3, '13': 4,\n",
      "                            '15': 5, '15g': 6, '18': 7, '20': 8, '2000': 9,\n",
      "                            '2005': 10, '2160': 11, '24': 12, '2mp': 13,\n",
      "                            '325': 14, '350': 15, '375': 16, '3o': 17, '42': 18,\n",
      "                            '44': 19, '45': 20, '4s': 21, '50': 22, '5020': 23,\n",
      "                            '510': 24, '5320': 25, '680': 26, '700w': 27,\n",
      "                            '8125': 28, '8525': 29, ...})\n",
      "Модель для классификации - LogisticRegression(C=3.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "Accuracy = 0.8080405454151937\n",
      "===========================\n",
      "Векторизация - CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None,\n",
      "                vocabulary={'10': 0, '100': 1, '11': 2, '12': 3, '13': 4,\n",
      "                            '15': 5, '15g': 6, '18': 7, '20': 8, '2000': 9,\n",
      "                            '2005': 10, '2160': 11, '24': 12, '2mp': 13,\n",
      "                            '325': 14, '350': 15, '375': 16, '3o': 17, '42': 18,\n",
      "                            '44': 19, '45': 20, '4s': 21, '50': 22, '5020': 23,\n",
      "                            '510': 24, '5320': 25, '680': 26, '700w': 27,\n",
      "                            '8125': 28, '8525': 29, ...})\n",
      "Модель для классификации - LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "          verbose=0)\n",
      "Accuracy = 0.8250667340018758\n",
      "===========================\n",
      "Векторизация - CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None,\n",
      "                vocabulary={'10': 0, '100': 1, '11': 2, '12': 3, '13': 4,\n",
      "                            '15': 5, '15g': 6, '18': 7, '20': 8, '2000': 9,\n",
      "                            '2005': 10, '2160': 11, '24': 12, '2mp': 13,\n",
      "                            '325': 14, '350': 15, '375': 16, '3o': 17, '42': 18,\n",
      "                            '44': 19, '45': 20, '4s': 21, '50': 22, '5020': 23,\n",
      "                            '510': 24, '5320': 25, '680': 26, '700w': 27,\n",
      "                            '8125': 28, '8525': 29, ...})\n",
      "Модель для классификации - KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n",
      "Accuracy = 0.6589952624870741\n",
      "===========================\n",
      "Векторизация - TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use_idf=True,\n",
      "                vocabulary={'10': 0, '100': 1, '11': 2, '12': 3, '13': 4,\n",
      "                            '15': 5, '15g': 6, '18': 7, '20': 8, '2000': 9,\n",
      "                            '2005': 10, '2160': 11, '24': 12, '2mp': 13,\n",
      "                            '325': 14, '350': 15, '375': 16, '3o': 17, '42': 18,\n",
      "                            '44': 19, '45': 20, '4s': 21, '50': 22, '5020': 23,\n",
      "                            '510': 24, '5320': 25, '680': 26, '700w': 27,\n",
      "                            '8125': 28, '8525': 29, ...})\n",
      "Модель для классификации - LogisticRegression(C=3.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "Accuracy = 0.8120445855277397\n",
      "===========================\n",
      "Векторизация - TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use_idf=True,\n",
      "                vocabulary={'10': 0, '100': 1, '11': 2, '12': 3, '13': 4,\n",
      "                            '15': 5, '15g': 6, '18': 7, '20': 8, '2000': 9,\n",
      "                            '2005': 10, '2160': 11, '24': 12, '2mp': 13,\n",
      "                            '325': 14, '350': 15, '375': 16, '3o': 17, '42': 18,\n",
      "                            '44': 19, '45': 20, '4s': 21, '50': 22, '5020': 23,\n",
      "                            '510': 24, '5320': 25, '680': 26, '700w': 27,\n",
      "                            '8125': 28, '8525': 29, ...})\n",
      "Модель для классификации - LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "          verbose=0)\n",
      "Accuracy = 0.8110405694634827\n",
      "===========================\n",
      "Векторизация - TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use_idf=True,\n",
      "                vocabulary={'10': 0, '100': 1, '11': 2, '12': 3, '13': 4,\n",
      "                            '15': 5, '15g': 6, '18': 7, '20': 8, '2000': 9,\n",
      "                            '2005': 10, '2160': 11, '24': 12, '2mp': 13,\n",
      "                            '325': 14, '350': 15, '375': 16, '3o': 17, '42': 18,\n",
      "                            '44': 19, '45': 20, '4s': 21, '50': 22, '5020': 23,\n",
      "                            '510': 24, '5320': 25, '680': 26, '700w': 27,\n",
      "                            '8125': 28, '8525': 29, ...})\n",
      "Модель для классификации - KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n",
      "Accuracy = 0.7720402568357261\n",
      "===========================\n"
     ]
    }
   ],
   "source": [
    "vectorizers_list = [CountVectorizer(vocabulary = corpusVocab), TfidfVectorizer(vocabulary = corpusVocab)]\n",
    "classifiers_list = [LogisticRegression(C=3.0), LinearSVC(), KNeighborsClassifier()]\n",
    "VectorizeAndClassify(vectorizers_list, classifiers_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dсе модели имеют посредственные показатели качества в районе 0,8.\n",
    "\n",
    "Лучшая точно была получения при использовании CountVectorizer с методом линеаризации SVC. Точность: 0.825."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Разделим выборку на обучающую и тестовую и проверим решение для лучшей модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(v, c):\n",
    "    model = Pipeline(\n",
    "        [(\"vectorizer\", v), \n",
    "         (\"classifier\", c)])\n",
    "    model.fit(X_train, Y_train)\n",
    "    Y_pred = model.predict(X_test)\n",
    "    print_accuracy_score_for_classes(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(data['text'],data['value'],test_size=0.05,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метка \t Accuracy\n",
      "0 \t 0.9\n",
      "1 \t 0.85\n"
     ]
    }
   ],
   "source": [
    "sentiment(CountVectorizer(), LinearSVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метка \t Accuracy\n",
      "0 \t 0.9333333333333333\n",
      "1 \t 0.85\n"
     ]
    }
   ],
   "source": [
    "sentiment(CountVectorizer(ngram_range=(1,3)), LinearSVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метка \t Accuracy\n",
      "0 \t 0.8666666666666667\n",
      "1 \t 0.75\n"
     ]
    }
   ],
   "source": [
    "sentiment(CountVectorizer(ngram_range=(2,3)), LinearSVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метка \t Accuracy\n",
      "0 \t 0.9333333333333333\n",
      "1 \t 0.85\n"
     ]
    }
   ],
   "source": [
    "sentiment(CountVectorizer(ngram_range=(1,4)), LinearSVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метка \t Accuracy\n",
      "0 \t 0.8\n",
      "1 \t 0.95\n"
     ]
    }
   ],
   "source": [
    "sentiment(CountVectorizer(ngram_range=(2,4)), LinearSVC())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучше всего себя показала комбинация векторизатора со словосочетаниями (1, 3) и (1, 4). Также видно, что при удалении из N-грам словосочетаний длиной 1 точность падает.\n",
    "\n",
    "Скорее всего это связано с тем, что оттенок отзыва лучше отражается словосочетанием, либо отдельными. но сильно выразительными словами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Решение задачи классификации текстов с использованием Байесовских методов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим распределение целевого признака"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(data['value'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Положительных и отрицательных отзывов поровну, поэтому будем MultinomialNB и ComplementNB должны работать примерно одинаково. Проверим это и сравним их"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метка \t Accuracy\n",
      "0 \t 0.9\n",
      "1 \t 0.85\n"
     ]
    }
   ],
   "source": [
    "sentiment(CountVectorizer(), MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метка \t Accuracy\n",
      "0 \t 0.8666666666666667\n",
      "1 \t 0.85\n"
     ]
    }
   ],
   "source": [
    "sentiment(TfidfVectorizer(), MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метка \t Accuracy\n",
      "0 \t 0.9\n",
      "1 \t 0.85\n"
     ]
    }
   ],
   "source": [
    "sentiment(CountVectorizer(), ComplementNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метка \t Accuracy\n",
      "0 \t 0.9\n",
      "1 \t 0.85\n"
     ]
    }
   ],
   "source": [
    "sentiment(TfidfVectorizer(), ComplementNB())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Точность получилась примерно одинаковой, как и ожидалось\n",
    "\n",
    "Точность для меток 0 и 1 не изменились по сравнению с CountVectorizer с методом опорных векторов. Наивный Байесовский метод показал настолько же точным, но не дал более высокого результата."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем другие методы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метка \t Accuracy\n",
      "0 \t 0.9\n",
      "1 \t 0.85\n"
     ]
    }
   ],
   "source": [
    "# Классификация с использованием логистической регрессии\n",
    "sentiment(CountVectorizer(), LogisticRegression(C=5.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метка \t Accuracy\n",
      "0 \t 0.9333333333333333\n",
      "1 \t 0.85\n"
     ]
    }
   ],
   "source": [
    "sentiment(TfidfVectorizer(), LogisticRegression(C=5.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метка \t Accuracy\n",
      "0 \t 0.9\n",
      "1 \t 0.8\n"
     ]
    }
   ],
   "source": [
    "sentiment(CountVectorizer(binary=True), BernoulliNB())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Методы классификации текстов, основанные на Наивном Байесе работают однозначно не хуже чем логистическая регрессия\n",
    "\n",
    "Он может себя хорошо показать, когда признаком текста будут являться целые числа или даже действительные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
